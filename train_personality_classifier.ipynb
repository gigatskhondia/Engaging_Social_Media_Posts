{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout,Input,Conv1D,MaxPooling1D,Flatten,GRU,Embedding\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json('./personality_captions/train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>image_hash</th>\n",
       "      <th>personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The snow will last as long as my sadness</td>\n",
       "      <td>1e22a9cf867d718551386b427c3b6d18</td>\n",
       "      <td>Intense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love experiencing new cultures</td>\n",
       "      <td>96472caea58db27769f1c282e2ac0</td>\n",
       "      <td>Adventurous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Look at that smooth easy catch of the ball. li...</td>\n",
       "      <td>f09d8fb76822158de129acb0fef463</td>\n",
       "      <td>Mellow (Soothing, Sweet)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I wish I could just run down this shore!</td>\n",
       "      <td>6e4ccc739ff44ed11da20ad9892317</td>\n",
       "      <td>Zany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psh, I've seen way better floral arrangements....</td>\n",
       "      <td>e7e1844aa9e67cddc6ffe8804d76e45b</td>\n",
       "      <td>Narcissistic (Self-centered, Egotistical)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0           The snow will last as long as my sadness   \n",
       "1                   I love experiencing new cultures   \n",
       "2  Look at that smooth easy catch of the ball. li...   \n",
       "3           I wish I could just run down this shore!   \n",
       "4  Psh, I've seen way better floral arrangements....   \n",
       "\n",
       "                         image_hash                                personality  \n",
       "0  1e22a9cf867d718551386b427c3b6d18                                    Intense  \n",
       "1     96472caea58db27769f1c282e2ac0                                Adventurous  \n",
       "2    f09d8fb76822158de129acb0fef463                   Mellow (Soothing, Sweet)  \n",
       "3    6e4ccc739ff44ed11da20ad9892317                                       Zany  \n",
       "4  e7e1844aa9e67cddc6ffe8804d76e45b  Narcissistic (Self-centered, Egotistical)  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['personality'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186858"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['personality'] = df['personality'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>image_hash</th>\n",
       "      <th>personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The snow will last as long as my sadness</td>\n",
       "      <td>1e22a9cf867d718551386b427c3b6d18</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love experiencing new cultures</td>\n",
       "      <td>96472caea58db27769f1c282e2ac0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Look at that smooth easy catch of the ball. li...</td>\n",
       "      <td>f09d8fb76822158de129acb0fef463</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I wish I could just run down this shore!</td>\n",
       "      <td>6e4ccc739ff44ed11da20ad9892317</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psh, I've seen way better floral arrangements....</td>\n",
       "      <td>e7e1844aa9e67cddc6ffe8804d76e45b</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  \\\n",
       "0           The snow will last as long as my sadness   \n",
       "1                   I love experiencing new cultures   \n",
       "2  Look at that smooth easy catch of the ball. li...   \n",
       "3           I wish I could just run down this shore!   \n",
       "4  Psh, I've seen way better floral arrangements....   \n",
       "\n",
       "                         image_hash  personality  \n",
       "0  1e22a9cf867d718551386b427c3b6d18          120  \n",
       "1     96472caea58db27769f1c282e2ac0            2  \n",
       "2    f09d8fb76822158de129acb0fef463          130  \n",
       "3    6e4ccc739ff44ed11da20ad9892317          216  \n",
       "4  e7e1844aa9e67cddc6ffe8804d76e45b          138  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=df.personality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj.fit_on_texts(df.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=max([len(s.split())for s in df.comment])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(tokenizer_obj.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens=tokenizer_obj.texts_to_sequences(df.comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad=pad_sequences(x_train_tokens, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create model ... \n"
     ]
    }
   ],
   "source": [
    "print (\"Create model ... \")\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size,EMBEDDING_DIM, input_length=max_length))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(500, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "#     model.add(Dense(350, activation='relu'))\n",
    "#     model.add(Dropout(0.3))    \n",
    "    model.add(Dense(217, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',f1_m])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compile model ...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 114, 150)          5092650   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 17100)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               8550500   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 217)               108717    \n",
      "=================================================================\n",
      "Total params: 14,002,367\n",
      "Trainable params: 14,002,367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "186858/186858 [==============================] - 14s 74us/step - loss: 5.2482 - acc: 0.0135 - f1_m: 1.9179e-04\n",
      "Epoch 2/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 4.7493 - acc: 0.0498 - f1_m: 0.0024\n",
      "Epoch 3/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 4.3439 - acc: 0.0873 - f1_m: 0.0114\n",
      "Epoch 4/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 3.9079 - acc: 0.1400 - f1_m: 0.0372\n",
      "Epoch 5/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 3.4689 - acc: 0.2101 - f1_m: 0.1017\n",
      "Epoch 6/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 3.0670 - acc: 0.2871 - f1_m: 0.2050\n",
      "Epoch 7/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 2.7326 - acc: 0.3566 - f1_m: 0.3114\n",
      "Epoch 8/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 2.4627 - acc: 0.4130 - f1_m: 0.3968\n",
      "Epoch 9/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 2.2489 - acc: 0.4576 - f1_m: 0.4605\n",
      "Epoch 10/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 2.0780 - acc: 0.4949 - f1_m: 0.5076\n",
      "Epoch 11/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.9373 - acc: 0.5254 - f1_m: 0.5446\n",
      "Epoch 12/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.8237 - acc: 0.5495 - f1_m: 0.5727\n",
      "Epoch 13/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.7233 - acc: 0.5701 - f1_m: 0.5958\n",
      "Epoch 14/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.6385 - acc: 0.5905 - f1_m: 0.6158\n",
      "Epoch 15/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.5636 - acc: 0.6047 - f1_m: 0.6319\n",
      "Epoch 16/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.4985 - acc: 0.6191 - f1_m: 0.6459\n",
      "Epoch 17/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.4459 - acc: 0.6313 - f1_m: 0.6596\n",
      "Epoch 18/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.3882 - acc: 0.6456 - f1_m: 0.6727\n",
      "Epoch 19/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.3413 - acc: 0.6557 - f1_m: 0.68160s - loss: 1.3348 - acc: 0\n",
      "Epoch 20/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.3000 - acc: 0.6647 - f1_m: 0.6916\n",
      "Epoch 21/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.2617 - acc: 0.6728 - f1_m: 0.6989\n",
      "Epoch 22/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.2268 - acc: 0.6810 - f1_m: 0.7072\n",
      "Epoch 23/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.1937 - acc: 0.6891 - f1_m: 0.7158\n",
      "Epoch 24/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.1687 - acc: 0.6952 - f1_m: 0.7206\n",
      "Epoch 25/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.1374 - acc: 0.7016 - f1_m: 0.7268\n",
      "Epoch 26/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.1190 - acc: 0.7068 - f1_m: 0.7311\n",
      "Epoch 27/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.0881 - acc: 0.7130 - f1_m: 0.7376\n",
      "Epoch 28/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 1.0663 - acc: 0.7190 - f1_m: 0.7427\n",
      "Epoch 29/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 1.0521 - acc: 0.7221 - f1_m: 0.7464\n",
      "Epoch 30/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 1.0286 - acc: 0.7280 - f1_m: 0.7512\n",
      "Epoch 31/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 1.0146 - acc: 0.7319 - f1_m: 0.7545\n",
      "Epoch 32/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.9972 - acc: 0.7361 - f1_m: 0.7590\n",
      "Epoch 33/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.9783 - acc: 0.7403 - f1_m: 0.7631\n",
      "Epoch 34/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.9664 - acc: 0.7426 - f1_m: 0.7650\n",
      "Epoch 35/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.9566 - acc: 0.7449 - f1_m: 0.7666\n",
      "Epoch 36/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.9389 - acc: 0.7497 - f1_m: 0.7708\n",
      "Epoch 37/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.9313 - acc: 0.7501 - f1_m: 0.7717\n",
      "Epoch 38/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.9190 - acc: 0.7552 - f1_m: 0.7763\n",
      "Epoch 39/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.9063 - acc: 0.7571 - f1_m: 0.7779\n",
      "Epoch 40/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.8914 - acc: 0.7614 - f1_m: 0.7819\n",
      "Epoch 41/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.8905 - acc: 0.7621 - f1_m: 0.7825\n",
      "Epoch 42/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.8780 - acc: 0.7653 - f1_m: 0.7856\n",
      "Epoch 43/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.8654 - acc: 0.7684 - f1_m: 0.7883\n",
      "Epoch 44/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.8572 - acc: 0.7712 - f1_m: 0.7905\n",
      "Epoch 45/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.8477 - acc: 0.7730 - f1_m: 0.7926\n",
      "Epoch 46/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.8465 - acc: 0.7732 - f1_m: 0.7931\n",
      "Epoch 47/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.8358 - acc: 0.7757 - f1_m: 0.7955\n",
      "Epoch 48/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.8267 - acc: 0.7780 - f1_m: 0.7976\n",
      "Epoch 49/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.8213 - acc: 0.7803 - f1_m: 0.7993\n",
      "Epoch 50/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.8097 - acc: 0.7828 - f1_m: 0.8016\n",
      "Epoch 51/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.8040 - acc: 0.7841 - f1_m: 0.8035\n",
      "Epoch 52/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.8032 - acc: 0.7846 - f1_m: 0.8027\n",
      "Epoch 53/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7956 - acc: 0.7857 - f1_m: 0.8048\n",
      "Epoch 54/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7830 - acc: 0.7889 - f1_m: 0.8074\n",
      "Epoch 55/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7843 - acc: 0.7889 - f1_m: 0.8078\n",
      "Epoch 56/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7775 - acc: 0.7910 - f1_m: 0.8088\n",
      "Epoch 57/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7749 - acc: 0.7921 - f1_m: 0.8105\n",
      "Epoch 58/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7660 - acc: 0.7936 - f1_m: 0.8119\n",
      "Epoch 59/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7591 - acc: 0.7954 - f1_m: 0.8136\n",
      "Epoch 60/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7585 - acc: 0.7948 - f1_m: 0.8128\n",
      "Epoch 61/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7487 - acc: 0.7994 - f1_m: 0.8169\n",
      "Epoch 62/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7481 - acc: 0.7986 - f1_m: 0.8163\n",
      "Epoch 63/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7397 - acc: 0.8015 - f1_m: 0.8185\n",
      "Epoch 64/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7439 - acc: 0.8003 - f1_m: 0.8180\n",
      "Epoch 65/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7318 - acc: 0.8028 - f1_m: 0.8198\n",
      "Epoch 66/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7348 - acc: 0.8022 - f1_m: 0.8208\n",
      "Epoch 67/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.7214 - acc: 0.8058 - f1_m: 0.8235\n",
      "Epoch 68/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.7176 - acc: 0.8069 - f1_m: 0.8241\n",
      "Epoch 69/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.7166 - acc: 0.8077 - f1_m: 0.8245\n",
      "Epoch 70/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.7147 - acc: 0.8087 - f1_m: 0.8253\n",
      "Epoch 71/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.7183 - acc: 0.8073 - f1_m: 0.8239\n",
      "Epoch 72/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.7073 - acc: 0.8105 - f1_m: 0.8266\n",
      "Epoch 73/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6980 - acc: 0.8116 - f1_m: 0.8281\n",
      "Epoch 74/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6995 - acc: 0.8130 - f1_m: 0.8294\n",
      "Epoch 75/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6970 - acc: 0.8127 - f1_m: 0.8298\n",
      "Epoch 76/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6972 - acc: 0.8130 - f1_m: 0.8293\n",
      "Epoch 77/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.6870 - acc: 0.8154 - f1_m: 0.8313\n",
      "Epoch 78/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6937 - acc: 0.8144 - f1_m: 0.8310\n",
      "Epoch 79/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.6881 - acc: 0.8144 - f1_m: 0.8310\n",
      "Epoch 80/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6782 - acc: 0.8183 - f1_m: 0.8342\n",
      "Epoch 81/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6770 - acc: 0.8177 - f1_m: 0.8339\n",
      "Epoch 82/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6777 - acc: 0.8175 - f1_m: 0.8333\n",
      "Epoch 83/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6768 - acc: 0.8182 - f1_m: 0.8340\n",
      "Epoch 84/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6699 - acc: 0.8192 - f1_m: 0.8351\n",
      "Epoch 85/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6654 - acc: 0.8209 - f1_m: 0.8368\n",
      "Epoch 86/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6640 - acc: 0.8217 - f1_m: 0.8368\n",
      "Epoch 87/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.6609 - acc: 0.8221 - f1_m: 0.8379\n",
      "Epoch 88/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6565 - acc: 0.8239 - f1_m: 0.8395\n",
      "Epoch 89/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.6584 - acc: 0.8229 - f1_m: 0.8391\n",
      "Epoch 90/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.6560 - acc: 0.8246 - f1_m: 0.8394\n",
      "Epoch 91/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6534 - acc: 0.8249 - f1_m: 0.8406\n",
      "Epoch 92/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6455 - acc: 0.8269 - f1_m: 0.8421\n",
      "Epoch 93/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.6487 - acc: 0.8265 - f1_m: 0.8420\n",
      "Epoch 94/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6483 - acc: 0.8270 - f1_m: 0.8419\n",
      "Epoch 95/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6385 - acc: 0.8298 - f1_m: 0.8448\n",
      "Epoch 96/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6472 - acc: 0.8276 - f1_m: 0.8425\n",
      "Epoch 97/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6348 - acc: 0.8294 - f1_m: 0.8447\n",
      "Epoch 98/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6357 - acc: 0.8304 - f1_m: 0.8453\n",
      "Epoch 99/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6347 - acc: 0.8305 - f1_m: 0.8454\n",
      "Epoch 100/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6286 - acc: 0.8316 - f1_m: 0.8463\n",
      "Epoch 101/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.6351 - acc: 0.8313 - f1_m: 0.8459\n",
      "Epoch 102/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.6329 - acc: 0.8307 - f1_m: 0.8458\n",
      "Epoch 103/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6292 - acc: 0.8320 - f1_m: 0.8468\n",
      "Epoch 104/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6275 - acc: 0.8325 - f1_m: 0.8468\n",
      "Epoch 105/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6180 - acc: 0.8345 - f1_m: 0.8488\n",
      "Epoch 106/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.6224 - acc: 0.8341 - f1_m: 0.8489\n",
      "Epoch 107/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6220 - acc: 0.8338 - f1_m: 0.8488\n",
      "Epoch 108/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6201 - acc: 0.8339 - f1_m: 0.8484\n",
      "Epoch 109/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6154 - acc: 0.8349 - f1_m: 0.8495\n",
      "Epoch 110/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6170 - acc: 0.8352 - f1_m: 0.8499\n",
      "Epoch 111/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6145 - acc: 0.8363 - f1_m: 0.8506\n",
      "Epoch 112/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6113 - acc: 0.8364 - f1_m: 0.8510\n",
      "Epoch 113/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6133 - acc: 0.8364 - f1_m: 0.8507\n",
      "Epoch 114/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6108 - acc: 0.8368 - f1_m: 0.8515\n",
      "Epoch 115/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6060 - acc: 0.8388 - f1_m: 0.8528\n",
      "Epoch 116/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.6038 - acc: 0.8390 - f1_m: 0.8531\n",
      "Epoch 117/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5986 - acc: 0.8396 - f1_m: 0.8539\n",
      "Epoch 118/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6035 - acc: 0.8386 - f1_m: 0.8532\n",
      "Epoch 119/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.6026 - acc: 0.8391 - f1_m: 0.8534\n",
      "Epoch 120/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5989 - acc: 0.8407 - f1_m: 0.8556\n",
      "Epoch 121/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5948 - acc: 0.8405 - f1_m: 0.8555\n",
      "Epoch 122/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5981 - acc: 0.8414 - f1_m: 0.8550\n",
      "Epoch 123/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5951 - acc: 0.8418 - f1_m: 0.8559\n",
      "Epoch 124/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5915 - acc: 0.8420 - f1_m: 0.8561\n",
      "Epoch 125/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5979 - acc: 0.8419 - f1_m: 0.8558\n",
      "Epoch 126/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5930 - acc: 0.8422 - f1_m: 0.8563\n",
      "Epoch 127/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5897 - acc: 0.8429 - f1_m: 0.8562\n",
      "Epoch 128/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5905 - acc: 0.8429 - f1_m: 0.8571\n",
      "Epoch 129/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5854 - acc: 0.8443 - f1_m: 0.8579\n",
      "Epoch 130/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5821 - acc: 0.8447 - f1_m: 0.8579\n",
      "Epoch 131/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5839 - acc: 0.8444 - f1_m: 0.8582\n",
      "Epoch 132/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5838 - acc: 0.8445 - f1_m: 0.8581\n",
      "Epoch 133/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5768 - acc: 0.8463 - f1_m: 0.8597\n",
      "Epoch 134/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5806 - acc: 0.8462 - f1_m: 0.8600\n",
      "Epoch 135/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5822 - acc: 0.8452 - f1_m: 0.8591\n",
      "Epoch 136/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5819 - acc: 0.8461 - f1_m: 0.8595\n",
      "Epoch 137/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5734 - acc: 0.8467 - f1_m: 0.8599\n",
      "Epoch 138/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5772 - acc: 0.8469 - f1_m: 0.8601\n",
      "Epoch 139/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5698 - acc: 0.8483 - f1_m: 0.8614\n",
      "Epoch 140/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5726 - acc: 0.8483 - f1_m: 0.8614\n",
      "Epoch 141/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5727 - acc: 0.8473 - f1_m: 0.8615\n",
      "Epoch 142/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.5682 - acc: 0.8483 - f1_m: 0.8619\n",
      "Epoch 143/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5696 - acc: 0.8500 - f1_m: 0.8632\n",
      "Epoch 144/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5654 - acc: 0.8497 - f1_m: 0.8623\n",
      "Epoch 145/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5715 - acc: 0.8479 - f1_m: 0.8617\n",
      "Epoch 146/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5685 - acc: 0.8498 - f1_m: 0.8626\n",
      "Epoch 147/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.5654 - acc: 0.8506 - f1_m: 0.8635\n",
      "Epoch 148/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5619 - acc: 0.8507 - f1_m: 0.8638\n",
      "Epoch 149/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5645 - acc: 0.8500 - f1_m: 0.8638\n",
      "Epoch 150/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5592 - acc: 0.8519 - f1_m: 0.8653\n",
      "Epoch 151/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5651 - acc: 0.8512 - f1_m: 0.8642\n",
      "Epoch 152/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5633 - acc: 0.8515 - f1_m: 0.8645\n",
      "Epoch 153/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5578 - acc: 0.8519 - f1_m: 0.8654\n",
      "Epoch 154/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5576 - acc: 0.8521 - f1_m: 0.8654\n",
      "Epoch 155/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5583 - acc: 0.8528 - f1_m: 0.8660\n",
      "Epoch 156/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5574 - acc: 0.8527 - f1_m: 0.8661\n",
      "Epoch 157/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5566 - acc: 0.8527 - f1_m: 0.8658\n",
      "Epoch 158/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5560 - acc: 0.8541 - f1_m: 0.8667\n",
      "Epoch 159/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5526 - acc: 0.8538 - f1_m: 0.8670\n",
      "Epoch 160/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5590 - acc: 0.8520 - f1_m: 0.8650\n",
      "Epoch 161/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5498 - acc: 0.8544 - f1_m: 0.8678\n",
      "Epoch 162/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5549 - acc: 0.8534 - f1_m: 0.8666\n",
      "Epoch 163/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5451 - acc: 0.8561 - f1_m: 0.8689\n",
      "Epoch 164/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5480 - acc: 0.8546 - f1_m: 0.8677\n",
      "Epoch 165/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5549 - acc: 0.8538 - f1_m: 0.8668\n",
      "Epoch 166/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5557 - acc: 0.8539 - f1_m: 0.8669\n",
      "Epoch 167/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5428 - acc: 0.8573 - f1_m: 0.8693\n",
      "Epoch 168/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5473 - acc: 0.8556 - f1_m: 0.8683\n",
      "Epoch 169/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5395 - acc: 0.8576 - f1_m: 0.8700\n",
      "Epoch 170/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5448 - acc: 0.8576 - f1_m: 0.8701\n",
      "Epoch 171/1000\n",
      "186858/186858 [==============================] - 12s 67us/step - loss: 0.5465 - acc: 0.8567 - f1_m: 0.8693\n",
      "Epoch 172/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5404 - acc: 0.8578 - f1_m: 0.8705\n",
      "Epoch 173/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5425 - acc: 0.8571 - f1_m: 0.8698\n",
      "Epoch 174/1000\n",
      "186858/186858 [==============================] - 12s 67us/step - loss: 0.5424 - acc: 0.8567 - f1_m: 0.8693\n",
      "Epoch 175/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5397 - acc: 0.8580 - f1_m: 0.8709\n",
      "Epoch 176/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5398 - acc: 0.8581 - f1_m: 0.8709\n",
      "Epoch 177/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5345 - acc: 0.8590 - f1_m: 0.8713\n",
      "Epoch 178/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5394 - acc: 0.8588 - f1_m: 0.8716\n",
      "Epoch 179/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5397 - acc: 0.8585 - f1_m: 0.8707\n",
      "Epoch 180/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5344 - acc: 0.8593 - f1_m: 0.8714\n",
      "Epoch 181/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5296 - acc: 0.8598 - f1_m: 0.8725\n",
      "Epoch 182/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5319 - acc: 0.8603 - f1_m: 0.8730\n",
      "Epoch 183/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5350 - acc: 0.8592 - f1_m: 0.8720\n",
      "Epoch 184/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5355 - acc: 0.8587 - f1_m: 0.8715\n",
      "Epoch 185/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5267 - acc: 0.8611 - f1_m: 0.8734\n",
      "Epoch 186/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5323 - acc: 0.8597 - f1_m: 0.8718\n",
      "Epoch 187/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5294 - acc: 0.8605 - f1_m: 0.8733\n",
      "Epoch 188/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5279 - acc: 0.8610 - f1_m: 0.8736\n",
      "Epoch 189/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5338 - acc: 0.8606 - f1_m: 0.8728\n",
      "Epoch 190/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.5239 - acc: 0.8627 - f1_m: 0.8748\n",
      "Epoch 191/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5301 - acc: 0.8607 - f1_m: 0.8733\n",
      "Epoch 192/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5245 - acc: 0.8622 - f1_m: 0.8741\n",
      "Epoch 193/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5270 - acc: 0.8613 - f1_m: 0.8735\n",
      "Epoch 194/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5206 - acc: 0.8633 - f1_m: 0.8755\n",
      "Epoch 195/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5226 - acc: 0.8630 - f1_m: 0.8754\n",
      "Epoch 196/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5186 - acc: 0.8637 - f1_m: 0.8756\n",
      "Epoch 197/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5268 - acc: 0.8624 - f1_m: 0.8743\n",
      "Epoch 198/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5280 - acc: 0.8625 - f1_m: 0.8747\n",
      "Epoch 199/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5216 - acc: 0.8632 - f1_m: 0.8759\n",
      "Epoch 200/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5215 - acc: 0.8634 - f1_m: 0.8756\n",
      "Epoch 201/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5247 - acc: 0.8626 - f1_m: 0.8745\n",
      "Epoch 202/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5247 - acc: 0.8630 - f1_m: 0.8754\n",
      "Epoch 203/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5220 - acc: 0.8635 - f1_m: 0.8756\n",
      "Epoch 204/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5176 - acc: 0.8644 - f1_m: 0.8765\n",
      "Epoch 205/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5233 - acc: 0.8633 - f1_m: 0.8755\n",
      "Epoch 206/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.5207 - acc: 0.8649 - f1_m: 0.8764\n",
      "Epoch 207/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5116 - acc: 0.8664 - f1_m: 0.8785\n",
      "Epoch 208/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5184 - acc: 0.8644 - f1_m: 0.8762\n",
      "Epoch 209/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5166 - acc: 0.8654 - f1_m: 0.8774\n",
      "Epoch 210/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.5130 - acc: 0.8663 - f1_m: 0.8780\n",
      "Epoch 211/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5154 - acc: 0.8649 - f1_m: 0.8766\n",
      "Epoch 212/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5138 - acc: 0.8663 - f1_m: 0.8781\n",
      "Epoch 213/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5165 - acc: 0.8660 - f1_m: 0.8781\n",
      "Epoch 214/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5155 - acc: 0.8655 - f1_m: 0.8779\n",
      "Epoch 215/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5118 - acc: 0.8657 - f1_m: 0.8785\n",
      "Epoch 216/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5099 - acc: 0.8667 - f1_m: 0.8787\n",
      "Epoch 217/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5143 - acc: 0.8661 - f1_m: 0.8780\n",
      "Epoch 218/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5110 - acc: 0.8674 - f1_m: 0.8789\n",
      "Epoch 219/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5125 - acc: 0.8661 - f1_m: 0.8781\n",
      "Epoch 220/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5110 - acc: 0.8666 - f1_m: 0.8787\n",
      "Epoch 221/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5022 - acc: 0.8687 - f1_m: 0.8804\n",
      "Epoch 222/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5100 - acc: 0.8663 - f1_m: 0.8784\n",
      "Epoch 223/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.5056 - acc: 0.8677 - f1_m: 0.8798\n",
      "Epoch 224/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5124 - acc: 0.8672 - f1_m: 0.8793\n",
      "Epoch 225/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5081 - acc: 0.8669 - f1_m: 0.8795\n",
      "Epoch 226/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5134 - acc: 0.8670 - f1_m: 0.8785\n",
      "Epoch 227/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5012 - acc: 0.8687 - f1_m: 0.8803\n",
      "Epoch 228/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5069 - acc: 0.8685 - f1_m: 0.8804\n",
      "Epoch 229/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5062 - acc: 0.8676 - f1_m: 0.8795\n",
      "Epoch 230/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.5031 - acc: 0.8685 - f1_m: 0.8799\n",
      "Epoch 231/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5029 - acc: 0.8692 - f1_m: 0.8808\n",
      "Epoch 232/1000\n",
      "186858/186858 [==============================] - 12s 63us/step - loss: 0.5010 - acc: 0.8696 - f1_m: 0.8810\n",
      "Epoch 233/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5000 - acc: 0.8694 - f1_m: 0.8807\n",
      "Epoch 234/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.5014 - acc: 0.8689 - f1_m: 0.8808\n",
      "Epoch 235/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5014 - acc: 0.8702 - f1_m: 0.8816\n",
      "Epoch 236/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5007 - acc: 0.8699 - f1_m: 0.8814\n",
      "Epoch 237/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.4993 - acc: 0.8703 - f1_m: 0.8819\n",
      "Epoch 238/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4993 - acc: 0.8700 - f1_m: 0.8817\n",
      "Epoch 239/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.4984 - acc: 0.8706 - f1_m: 0.8819\n",
      "Epoch 240/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5040 - acc: 0.8689 - f1_m: 0.8810\n",
      "Epoch 241/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5064 - acc: 0.8697 - f1_m: 0.8810\n",
      "Epoch 242/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.4986 - acc: 0.8701 - f1_m: 0.8820\n",
      "Epoch 243/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.4995 - acc: 0.8702 - f1_m: 0.8818\n",
      "Epoch 244/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.4989 - acc: 0.8698 - f1_m: 0.8819\n",
      "Epoch 245/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.4913 - acc: 0.8724 - f1_m: 0.8839\n",
      "Epoch 246/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4959 - acc: 0.8707 - f1_m: 0.8827\n",
      "Epoch 247/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.5020 - acc: 0.8701 - f1_m: 0.8823\n",
      "Epoch 248/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4992 - acc: 0.8713 - f1_m: 0.8825\n",
      "Epoch 249/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4963 - acc: 0.8713 - f1_m: 0.8832\n",
      "Epoch 250/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4953 - acc: 0.8718 - f1_m: 0.8832\n",
      "Epoch 251/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.4957 - acc: 0.8723 - f1_m: 0.8837\n",
      "Epoch 252/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4926 - acc: 0.8721 - f1_m: 0.8836\n",
      "Epoch 253/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4939 - acc: 0.8723 - f1_m: 0.88370s - loss: 0.4938 - acc: 0.8724 - \n",
      "Epoch 254/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4903 - acc: 0.8735 - f1_m: 0.8853\n",
      "Epoch 255/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4948 - acc: 0.8725 - f1_m: 0.8840\n",
      "Epoch 256/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4953 - acc: 0.8717 - f1_m: 0.8831\n",
      "Epoch 257/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4950 - acc: 0.8726 - f1_m: 0.8839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 258/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4913 - acc: 0.8735 - f1_m: 0.8843\n",
      "Epoch 259/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.4923 - acc: 0.8729 - f1_m: 0.8841\n",
      "Epoch 260/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4853 - acc: 0.8740 - f1_m: 0.8854\n",
      "Epoch 261/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4923 - acc: 0.8720 - f1_m: 0.8837\n",
      "Epoch 262/1000\n",
      "186858/186858 [==============================] - 12s 64us/step - loss: 0.4917 - acc: 0.8731 - f1_m: 0.8843\n",
      "Epoch 263/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4883 - acc: 0.8735 - f1_m: 0.8848\n",
      "Epoch 264/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4947 - acc: 0.8723 - f1_m: 0.8835\n",
      "Epoch 265/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4869 - acc: 0.8743 - f1_m: 0.8856\n",
      "Epoch 266/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4923 - acc: 0.8733 - f1_m: 0.8845\n",
      "Epoch 267/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4858 - acc: 0.8746 - f1_m: 0.8856\n",
      "Epoch 268/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4939 - acc: 0.8734 - f1_m: 0.8846\n",
      "Epoch 269/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4877 - acc: 0.8741 - f1_m: 0.8850\n",
      "Epoch 270/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4867 - acc: 0.8743 - f1_m: 0.8853\n",
      "Epoch 271/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4863 - acc: 0.8752 - f1_m: 0.8864\n",
      "Epoch 272/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4884 - acc: 0.8746 - f1_m: 0.8858\n",
      "Epoch 273/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.4887 - acc: 0.8734 - f1_m: 0.8847\n",
      "Epoch 274/1000\n",
      "186858/186858 [==============================] - 13s 67us/step - loss: 0.4860 - acc: 0.8753 - f1_m: 0.8865\n",
      "Epoch 275/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4820 - acc: 0.8757 - f1_m: 0.8870\n",
      "Epoch 276/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4811 - acc: 0.8752 - f1_m: 0.8866\n",
      "Epoch 277/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4800 - acc: 0.8757 - f1_m: 0.8872\n",
      "Epoch 278/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.4860 - acc: 0.8753 - f1_m: 0.8872\n",
      "Epoch 279/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4870 - acc: 0.8749 - f1_m: 0.8864\n",
      "Epoch 280/1000\n",
      "186858/186858 [==============================] - 12s 66us/step - loss: 0.4778 - acc: 0.8766 - f1_m: 0.8880\n",
      "Epoch 281/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4844 - acc: 0.8763 - f1_m: 0.8873\n",
      "Epoch 282/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4841 - acc: 0.8757 - f1_m: 0.8869\n",
      "Epoch 283/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4881 - acc: 0.8753 - f1_m: 0.8870\n",
      "Epoch 284/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4825 - acc: 0.8751 - f1_m: 0.8863\n",
      "Epoch 285/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4814 - acc: 0.8758 - f1_m: 0.8872\n",
      "Epoch 286/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4761 - acc: 0.8778 - f1_m: 0.8890\n",
      "Epoch 287/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4876 - acc: 0.8749 - f1_m: 0.8862\n",
      "Epoch 288/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4814 - acc: 0.8761 - f1_m: 0.8874\n",
      "Epoch 289/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4774 - acc: 0.8771 - f1_m: 0.8884\n",
      "Epoch 290/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4801 - acc: 0.8769 - f1_m: 0.8884\n",
      "Epoch 291/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4810 - acc: 0.8763 - f1_m: 0.8875\n",
      "Epoch 292/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4799 - acc: 0.8773 - f1_m: 0.8885\n",
      "Epoch 293/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4766 - acc: 0.8768 - f1_m: 0.8881\n",
      "Epoch 294/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4791 - acc: 0.8781 - f1_m: 0.8888\n",
      "Epoch 295/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4793 - acc: 0.8777 - f1_m: 0.8886\n",
      "Epoch 296/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4772 - acc: 0.8773 - f1_m: 0.8887\n",
      "Epoch 297/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4864 - acc: 0.8758 - f1_m: 0.8873\n",
      "Epoch 298/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4733 - acc: 0.8784 - f1_m: 0.8890\n",
      "Epoch 299/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4783 - acc: 0.8776 - f1_m: 0.8888\n",
      "Epoch 300/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4793 - acc: 0.8774 - f1_m: 0.8884\n",
      "Epoch 301/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4765 - acc: 0.8785 - f1_m: 0.8894\n",
      "Epoch 302/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4750 - acc: 0.8791 - f1_m: 0.8897\n",
      "Epoch 303/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4698 - acc: 0.8791 - f1_m: 0.8904\n",
      "Epoch 304/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4759 - acc: 0.8780 - f1_m: 0.8890\n",
      "Epoch 305/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4797 - acc: 0.8778 - f1_m: 0.8891\n",
      "Epoch 306/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4816 - acc: 0.8769 - f1_m: 0.8881\n",
      "Epoch 307/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4769 - acc: 0.8783 - f1_m: 0.8894\n",
      "Epoch 308/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4702 - acc: 0.8792 - f1_m: 0.8899\n",
      "Epoch 309/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4721 - acc: 0.8788 - f1_m: 0.8902\n",
      "Epoch 310/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4746 - acc: 0.8790 - f1_m: 0.8899\n",
      "Epoch 311/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4792 - acc: 0.8788 - f1_m: 0.8898\n",
      "Epoch 312/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4764 - acc: 0.8787 - f1_m: 0.8900\n",
      "Epoch 313/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4776 - acc: 0.8788 - f1_m: 0.8898\n",
      "Epoch 314/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4790 - acc: 0.8774 - f1_m: 0.8889\n",
      "Epoch 315/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4694 - acc: 0.8797 - f1_m: 0.8911\n",
      "Epoch 316/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4742 - acc: 0.8789 - f1_m: 0.8901\n",
      "Epoch 317/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4736 - acc: 0.8786 - f1_m: 0.8901\n",
      "Epoch 318/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4728 - acc: 0.8799 - f1_m: 0.8913\n",
      "Epoch 319/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4700 - acc: 0.8801 - f1_m: 0.8914\n",
      "Epoch 320/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4705 - acc: 0.8801 - f1_m: 0.8913\n",
      "Epoch 321/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4716 - acc: 0.8795 - f1_m: 0.8904\n",
      "Epoch 322/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4665 - acc: 0.8817 - f1_m: 0.8919\n",
      "Epoch 323/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4745 - acc: 0.8791 - f1_m: 0.8901\n",
      "Epoch 324/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4808 - acc: 0.8786 - f1_m: 0.8896\n",
      "Epoch 325/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4707 - acc: 0.8816 - f1_m: 0.8923\n",
      "Epoch 326/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4682 - acc: 0.8806 - f1_m: 0.8912\n",
      "Epoch 327/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4692 - acc: 0.8808 - f1_m: 0.8916\n",
      "Epoch 328/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4671 - acc: 0.8805 - f1_m: 0.8912\n",
      "Epoch 329/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4700 - acc: 0.8812 - f1_m: 0.8916\n",
      "Epoch 330/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4676 - acc: 0.8815 - f1_m: 0.8918\n",
      "Epoch 331/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4682 - acc: 0.8812 - f1_m: 0.8920\n",
      "Epoch 332/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4664 - acc: 0.8816 - f1_m: 0.8927\n",
      "Epoch 333/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4716 - acc: 0.8806 - f1_m: 0.8915\n",
      "Epoch 334/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4718 - acc: 0.8800 - f1_m: 0.8903\n",
      "Epoch 335/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4701 - acc: 0.8806 - f1_m: 0.8908\n",
      "Epoch 336/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4667 - acc: 0.8811 - f1_m: 0.8919\n",
      "Epoch 337/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4661 - acc: 0.8819 - f1_m: 0.8928\n",
      "Epoch 338/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4682 - acc: 0.8825 - f1_m: 0.8930\n",
      "Epoch 339/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4709 - acc: 0.8807 - f1_m: 0.8913\n",
      "Epoch 340/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4726 - acc: 0.8812 - f1_m: 0.8918\n",
      "Epoch 341/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4664 - acc: 0.8818 - f1_m: 0.8923\n",
      "Epoch 342/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4645 - acc: 0.8821 - f1_m: 0.8930\n",
      "Epoch 343/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4661 - acc: 0.8819 - f1_m: 0.8924\n",
      "Epoch 344/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4694 - acc: 0.8805 - f1_m: 0.8919\n",
      "Epoch 345/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4686 - acc: 0.8809 - f1_m: 0.8917\n",
      "Epoch 346/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4637 - acc: 0.8833 - f1_m: 0.8936\n",
      "Epoch 347/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4643 - acc: 0.8836 - f1_m: 0.8939\n",
      "Epoch 348/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4646 - acc: 0.8820 - f1_m: 0.8930\n",
      "Epoch 349/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4650 - acc: 0.8828 - f1_m: 0.8934\n",
      "Epoch 350/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4648 - acc: 0.8823 - f1_m: 0.8931\n",
      "Epoch 351/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4678 - acc: 0.8817 - f1_m: 0.8925\n",
      "Epoch 352/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4644 - acc: 0.8828 - f1_m: 0.8933\n",
      "Epoch 353/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4657 - acc: 0.8824 - f1_m: 0.8931\n",
      "Epoch 354/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4676 - acc: 0.8824 - f1_m: 0.8934\n",
      "Epoch 355/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4684 - acc: 0.8815 - f1_m: 0.8927\n",
      "Epoch 356/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4626 - acc: 0.8832 - f1_m: 0.8941\n",
      "Epoch 357/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4617 - acc: 0.8831 - f1_m: 0.8937\n",
      "Epoch 358/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4648 - acc: 0.8830 - f1_m: 0.8932\n",
      "Epoch 359/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4638 - acc: 0.8826 - f1_m: 0.8935\n",
      "Epoch 360/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4630 - acc: 0.8815 - f1_m: 0.8927\n",
      "Epoch 361/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4624 - acc: 0.8826 - f1_m: 0.8931\n",
      "Epoch 362/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4674 - acc: 0.8823 - f1_m: 0.8932\n",
      "Epoch 363/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4682 - acc: 0.8822 - f1_m: 0.8929\n",
      "Epoch 364/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4618 - acc: 0.8828 - f1_m: 0.8938\n",
      "Epoch 365/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4657 - acc: 0.8822 - f1_m: 0.8933\n",
      "Epoch 366/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4624 - acc: 0.8841 - f1_m: 0.8948\n",
      "Epoch 367/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4626 - acc: 0.8832 - f1_m: 0.8935\n",
      "Epoch 368/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4599 - acc: 0.8835 - f1_m: 0.8944\n",
      "Epoch 369/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4603 - acc: 0.8844 - f1_m: 0.8945\n",
      "Epoch 370/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4625 - acc: 0.8833 - f1_m: 0.8939\n",
      "Epoch 371/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4597 - acc: 0.8842 - f1_m: 0.8949\n",
      "Epoch 372/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4677 - acc: 0.8834 - f1_m: 0.8939\n",
      "Epoch 373/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4635 - acc: 0.8839 - f1_m: 0.8945\n",
      "Epoch 374/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4617 - acc: 0.8833 - f1_m: 0.8940\n",
      "Epoch 375/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4588 - acc: 0.8857 - f1_m: 0.8958\n",
      "Epoch 376/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4597 - acc: 0.8845 - f1_m: 0.8950\n",
      "Epoch 377/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4552 - acc: 0.8848 - f1_m: 0.8954\n",
      "Epoch 378/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4624 - acc: 0.8842 - f1_m: 0.8948\n",
      "Epoch 379/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4570 - acc: 0.8848 - f1_m: 0.89550s - loss: 0.4555 - ac\n",
      "Epoch 380/1000\n",
      "186858/186858 [==============================] - 12s 65us/step - loss: 0.4619 - acc: 0.8835 - f1_m: 0.8941\n",
      "Epoch 381/1000\n",
      " 53504/186858 [=======>......................] - ETA: 8s - loss: 0.4417 - acc: 0.8883 - f1_m: 0.8992"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-e69a7def9071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/gpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Compile model ...\")\n",
    "estimator = KerasClassifier(build_fn=build_model, epochs=400, batch_size=256)\n",
    "with tf.device('/gpu:0'):\n",
    "    estimator.fit(x_train_pad, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv\n",
    "# kfold = KFold(n_splits=10, shuffle=True)\n",
    "# with tf.device('/gpu:0'):\n",
    "#     results = cross_val_score(estimator, train_x, train_y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = estimator.model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./keras_model/per_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.model.save_weights(\"./keras_model/per_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restoring model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('./keras_model/per_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"./keras_model/per_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_dic = {0: 'Abrasive (Annoying, Irritating)',\n",
    " 1: 'Absentminded',\n",
    " 2: 'Adventurous',\n",
    " 3: 'Aggressive',\n",
    " 4: 'Airy (Casual, Not Serious)',\n",
    " 5: 'Aloof (Detached, Distant)',\n",
    " 6: 'Amusing',\n",
    " 7: 'Angry',\n",
    " 8: 'Anxious',\n",
    " 9: 'Apathetic (Uncaring, Disinterested)',\n",
    " 10: 'Appreciative (Grateful)',\n",
    " 11: 'Argumentative',\n",
    " 12: 'Arrogant',\n",
    " 13: 'Artful',\n",
    " 14: 'Articulate (Well-spoken, Expressive)',\n",
    " 15: 'Artificial',\n",
    " 16: 'Assertive',\n",
    " 17: 'Attractive',\n",
    " 18: 'Barbaric',\n",
    " 19: 'Bewildered (Astonished, Confused)',\n",
    " 20: 'Bizarre',\n",
    " 21: 'Bland',\n",
    " 22: 'Blunt',\n",
    " 23: 'Boisterous (Rowdy, Loud)',\n",
    " 24: 'Boyish',\n",
    " 25: 'Breezy (Relaxed, Informal)',\n",
    " 26: 'Brilliant',\n",
    " 27: 'Businesslike',\n",
    " 28: 'Calm',\n",
    " 29: 'Captivating',\n",
    " 30: 'Caring',\n",
    " 31: 'Casual',\n",
    " 32: 'Cerebral (Intellectual, Logical)',\n",
    " 33: 'Charming',\n",
    " 34: 'Cheerful',\n",
    " 35: 'Childish',\n",
    " 36: 'Clever',\n",
    " 37: 'Coarse (Not Fine, Crass)',\n",
    " 38: 'Cold',\n",
    " 39: 'Colorful (Full of Life, Interesting)',\n",
    " 40: 'Compassionate (Sympathetic, Warm)',\n",
    " 41: 'Complex',\n",
    " 42: 'Conceited (Arrogant, Egotistical)',\n",
    " 43: 'Confident',\n",
    " 44: 'Confused',\n",
    " 45: 'Conservative (Traditional, Conventional)',\n",
    " 46: 'Considerate',\n",
    " 47: 'Contemplative (Reflective, Thoughtful)',\n",
    " 48: 'Contemptible (Despicable, Vile)',\n",
    " 49: 'Contradictory',\n",
    " 50: 'Courageous',\n",
    " 51: 'Cowardly',\n",
    " 52: 'Crazy',\n",
    " 53: 'Creative',\n",
    " 54: 'Critical',\n",
    " 55: 'Crude',\n",
    " 56: 'Cruel',\n",
    " 57: 'Cultured (Refined, Educated)',\n",
    " 58: 'Curious',\n",
    " 59: 'Cute',\n",
    " 60: 'Cynical (Doubtful, Skeptical)',\n",
    " 61: 'Daring',\n",
    " 62: 'Deep',\n",
    " 63: 'Destructive',\n",
    " 64: 'Devious',\n",
    " 65: 'Discouraging',\n",
    " 66: 'Disturbing',\n",
    " 67: 'Dramatic',\n",
    " 68: 'Dreamy',\n",
    " 69: 'Dry',\n",
    " 70: 'Dull',\n",
    " 71: 'Earnest',\n",
    " 72: 'Earnest (Enthusiastic)',\n",
    " 73: 'Egocentric (Self-centered)',\n",
    " 74: 'Elegant',\n",
    " 75: 'Eloquent (Well-spoken, Expressive)',\n",
    " 76: 'Emotional',\n",
    " 77: 'Empathetic',\n",
    " 78: 'Energetic',\n",
    " 79: 'Enigmatic (Cryptic, Obscure)',\n",
    " 80: 'Enthusiastic',\n",
    " 81: 'Envious',\n",
    " 82: 'Erratic',\n",
    " 83: 'Escapist (Dreamer, Seeks Distraction)',\n",
    " 84: 'Excitable',\n",
    " 85: 'Exciting',\n",
    " 86: 'Extraordinary',\n",
    " 87: 'Extravagant',\n",
    " 88: 'Extreme',\n",
    " 89: 'Fanatical',\n",
    " 90: 'Fanciful',\n",
    " 91: 'Fatalistic (Bleak, Gloomy)',\n",
    " 92: 'Fawning (Flattering, Deferential)',\n",
    " 93: 'Fearful',\n",
    " 94: 'Fickle (Changeable, Temperamental)',\n",
    " 95: 'Fiery',\n",
    " 96: 'Foolish',\n",
    " 97: 'Formal',\n",
    " 98: 'Freethinking',\n",
    " 99: 'Frightening',\n",
    " 100: 'Frivolous (Trivial, Silly)',\n",
    " 101: 'Fun-loving',\n",
    " 102: 'Gentle',\n",
    " 103: 'Glamorous',\n",
    " 104: 'Gloomy',\n",
    " 105: 'Grand',\n",
    " 106: 'Grim',\n",
    " 107: 'Happy',\n",
    " 108: 'Hateful',\n",
    " 109: 'Haughty (Arrogant, Snobbish)',\n",
    " 110: 'High-spirited',\n",
    " 111: 'Honest',\n",
    " 112: 'Hostile',\n",
    " 113: 'Humble',\n",
    " 114: 'Humorous',\n",
    " 115: 'Idealistic',\n",
    " 116: 'Imaginative',\n",
    " 117: 'Impersonal',\n",
    " 118: 'Insightful',\n",
    " 119: 'Intelligent',\n",
    " 120: 'Intense',\n",
    " 121: 'Irrational',\n",
    " 122: 'Irritable',\n",
    " 123: 'Kind',\n",
    " 124: 'Knowledgeable',\n",
    " 125: 'Lazy',\n",
    " 126: 'Logical',\n",
    " 127: 'Malicious',\n",
    " 128: 'Maternal (Mother-like)',\n",
    " 129: 'Melancholic',\n",
    " 130: 'Mellow (Soothing, Sweet)',\n",
    " 131: 'Meticulous (Precise, Thorough)',\n",
    " 132: 'Miserable',\n",
    " 133: 'Money-minded',\n",
    " 134: 'Monstrous',\n",
    " 135: 'Moody',\n",
    " 136: 'Morbid',\n",
    " 137: 'Mystical',\n",
    " 138: 'Narcissistic (Self-centered, Egotistical)',\n",
    " 139: 'Neurotic (Manic, Obsessive)',\n",
    " 140: 'Neutral',\n",
    " 141: 'Nihilistic',\n",
    " 142: 'Objective (Detached, Impartial)',\n",
    " 143: 'Obnoxious',\n",
    " 144: 'Observant',\n",
    " 145: 'Obsessive',\n",
    " 146: 'Odd',\n",
    " 147: 'Offhand',\n",
    " 148: 'Old-fashioned',\n",
    " 149: 'Open',\n",
    " 150: 'Opinionated',\n",
    " 151: 'Optimistic',\n",
    " 152: 'Ordinary',\n",
    " 153: 'Outrageous',\n",
    " 154: 'Overimaginative',\n",
    " 155: 'Paranoid',\n",
    " 156: 'Passionate',\n",
    " 157: 'Passive',\n",
    " 158: 'Patriotic',\n",
    " 159: 'Peaceful',\n",
    " 160: 'Perceptive',\n",
    " 161: 'Playful',\n",
    " 162: 'Pompous (Self-important, Arrogant)',\n",
    " 163: 'Practical',\n",
    " 164: 'Pretentious (Snobbish, Showy)',\n",
    " 165: 'Profound',\n",
    " 166: 'Provocative',\n",
    " 167: 'Questioning',\n",
    " 168: 'Quirky',\n",
    " 169: 'Rational',\n",
    " 170: 'Realistic',\n",
    " 171: 'Reflective',\n",
    " 172: 'Relaxed',\n",
    " 173: 'Resentful',\n",
    " 174: 'Respectful',\n",
    " 175: 'Ridiculous',\n",
    " 176: 'Rigid',\n",
    " 177: 'Romantic',\n",
    " 178: 'Rowdy',\n",
    " 179: 'Rustic (Rural)',\n",
    " 180: 'Sarcastic',\n",
    " 181: 'Scholarly',\n",
    " 182: 'Scornful',\n",
    " 183: 'Sensitive',\n",
    " 184: 'Sensual',\n",
    " 185: 'Sentimental',\n",
    " 186: 'Serious',\n",
    " 187: 'Shy',\n",
    " 188: 'Silly',\n",
    " 189: 'Simple',\n",
    " 190: 'Skeptical',\n",
    " 191: 'Solemn',\n",
    " 192: 'Sophisticated',\n",
    " 193: 'Spirited',\n",
    " 194: 'Spontaneous',\n",
    " 195: 'Stiff',\n",
    " 196: 'Stoic (Unemotional, Matter-of-fact)',\n",
    " 197: 'Stupid',\n",
    " 198: 'Stylish',\n",
    " 199: 'Suave (Charming, Smooth)',\n",
    " 200: 'Sweet',\n",
    " 201: 'Sympathetic',\n",
    " 202: 'Tense',\n",
    " 203: 'Tough',\n",
    " 204: 'Uncreative',\n",
    " 205: 'Unimaginative',\n",
    " 206: 'Unrealistic',\n",
    " 207: 'Vacuous (Empty, Unintelligent)',\n",
    " 208: 'Vague',\n",
    " 209: 'Vivacious (Lively, Animated)',\n",
    " 210: 'Warm',\n",
    " 211: 'Whimsical (Playful, Fanciful)',\n",
    " 212: 'Wise',\n",
    " 213: 'Wishful',\n",
    " 214: 'Witty',\n",
    " 215: 'Youthful',\n",
    " 216: 'Zany'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_sentence, persona_dic, loaded_model, tokenizer_obj):\n",
    "    sent=[test_sentence]\n",
    "    x_train_tokens=tokenizer_obj.texts_to_sequences(sent)\n",
    "    x_train_pad=pad_sequences(x_train_tokens, maxlen=114, padding='post')\n",
    "    result=loaded_model.predict(x=x_train_pad)\n",
    "    return persona_dic[np.argmax(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence='Nice beard my friend! want to compete with my'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Playful'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(test_sentence, persona_dic, loaded_model, tokenizer_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>image_hash</th>\n",
       "      <th>personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168488</th>\n",
       "      <td>Nice beard my friend! want to compete with my!?</td>\n",
       "      <td>c9caf6c098068c13eeb10e6266046d3</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  \\\n",
       "168488  Nice beard my friend! want to compete with my!?   \n",
       "\n",
       "                             image_hash  personality  \n",
       "168488  c9caf6c098068c13eeb10e6266046d3          161  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['comment'].str.contains(\"Nice beard my friend! want to compete with my\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
